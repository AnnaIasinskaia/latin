DATA STRUCTURES

Location Information (2D arrays, inner array for words, outer array for docs)
    wordLocationArray = [[word, word, word], [word,word,word]]
    topicAssignmentByLoc = [[tag, tag, tag], [tag, tag, tag]]

Word Information (indices line up)
    uniqueWordDict = {word:count, word:count, word:count} (not updated after init)
    wordDistributionAcrossTopics = [[count,count], [count,count]] (index of count in inner array corresponds to topic)

Topic Information (indices line up)
    topicWordInstancesDict = [{word:count, word:count}, {word:count,word:count}]
    topicTotalWordCount = [#wordsInTopic, #wordsInTopic]

Document Information (indices line up)
    docTopicalWordDist = [[#docWords, #docWords], [#docWords, #docWords]] (outer array is list of docs, inner array is #docwords in each topic)
    docTotalWordCounts = [#wordsInDoc, #wordsInDoc] (not updated after init)



METHODS

init
    Make the above data structures from input.

runLDA
    for i in range(0, iterations):
        for doc in wordLocationList:
            for word in doc:
                wordProbabilities = calculateProbabilities(word)
                updateDataStructures(word, doc, wordProbabilities)
    printTopics()

calculateProbabilities(word)
    newWordProbabilities = []
    for topic in topicList:
        use p(w|t) and p(t|d) to calculate p(t|w) (this is where hyperparameters happen)
        put probability in newWordProbabilities at index topic
    return newWordProbabilities

updateDataStructures(word, doc, wordProbabilities)
    update all data structures based on new probabilities
    get word's current topic using tagsByLocation
    topicList(decrement old location, increment new location)
    topicWordCounts(decrement/increment)
    docList(find doc we were just in, decrement/increment docWords in topic if needed, decrement/increment docWords in other doc if needed)
    tagsByLocation

printTopics()
    for topic in topicList:
        print(topic + "\n")



EFFICIENCY

n = number of words in corpus
i = number iterations
t = number topics

calculateProbabilities for 1 word runs in O(t) time
updateDataStructures for 1 word runs in O(1) time


worst case scenario: n = t (every word is its own topic)
1 iteration of runLDA runs in O(n^2) time